{
	# Configure default logging for all components using human-friendly format (console)
	log default {
		output stderr
		format console
		level DEBUG
	}
	# Configure a file as an additional logging output using JSON format
	# log file {
	# 	output file ./caddy.log
	# 	format json
	# 	level INFO
	# }
	# This line is required to tell our Web Server that authentication should happen BEFORE forwarding requests to handler
	# I'd like to remove it, but for now it is required
	order authorize_with before handle
	# This line is required to use a local certificate issuer rather than Let's Encrypt or ZeroSSL
	# Valid values are: letsencrypt, zerossl, internal
	cert_issuer internal
	# This enables Prometheus metrics for HTTP servers
	# I'd like to have this enabled by default, but for now it is required
	servers :443 {
		metrics
	}
	# This declares Azure Key Vault instances (secret stores) where secrets can be fetched
	# Those secrets can be used in our configuration using the syntax {secret.<secret_name>@<store_name>}
	secrets {
		store az-kv-srv-01 azure_keyvault https://kv-srv-01.vault.azure.net

		# Alternatively, it's possible to automate secret fetching using the following syntax
		# This will fetch the secret every hour, and pass it to a "file" handler which will
		# write the secret to a file and execute a command when the secret changes
		# This is useful to automatically update secrets in a systemd environment or a 
		# docker environment for example
		automate {
			source some-secret@az-kv-srv-01
			interval 1h
			template "The value is: {some-secret@az-kv-srv-01}"
			handle file {
				path ./secret.txt
				chmod 0600
				notify exec {
					command touch ./some-other-file
				}
			}
		}
	}
	# Start docker containers alongside the application
	# Networes are always created before starting containers
	docker {
		network mynetwork {
			driver bridge
			ipam {
				subnet 172.200.200.0/24
				gateway 172.200.200.1
			}
		}
		container demo-nginx {
			image docker.io/library/nginx:latest
			network mynetwork
			restart always
			mount /data/test to /data
			environment {
				ENV1 "value1"
				ENV2 "value2"
			}
			env ENV3 "value3"
			label com.quara-dev.service "test"
			label com.quara-dev.service.version "1.0.0"
			label com.quara-dev.service.env "dev"
		}
		container other {
			image docker.io/library/alpine:latest
			restart on-failure
		}
	}
	# Start python processes alongside the application
	# Each python process may be started in a virtual environment and may have a custom entrypoint
	# Alternatively, it's possible to specify a default virtual environment for all processes.
	# python {
	# 	virtualenv /home/charbonnierg/github/quara-dev/beyond/sandbox/.venv
	# 	app demo1 {
	# 		entrypoint /home/charbonnierg/github/quara-dev/beyond/sandbox/app1
	# 		forward_stdout
	# 		forward_stderr
	# 	}
	# 	app demo2 {
	# 		entrypoint /home/charbonnierg/github/quara-dev/beyond/sandbox/app2
	# 		forward_stdout
	# 		forward_stderr
	# 	}
	# }
	# This block is required to configure and start NATS server when our Web Server starts
	broker {
		cert_issuer internal
		account OAUTH2 {
			jetstream on
			authorize {
				match token OAUTH2
				match host 127.0.0.1
				callout allow
			}
		}
		auth_callout {
			account AUTH
		}
		account DATA {
			authorize {
				match token DATA
				match host 127.0.0.1
				callout allow
			}
			stream test {
				subjects "test.>"
			}
			flow {
				source mongodb_change_stream {
					uri "mongodb://localhost:27017"
					database test
					collection test1
				}
				destination stream {
					name "MONGOSTREAM"
					prefix "mongo"
				}
			}
			flow {
				source stream_consumer {
					stream "MONGOSTREAM"
					durable_name "db-sync"
				}
				destination mongodb_change_stream {
					uri "mongodb://localhost:27017"
					database test2
				}
			}
			flow {
				source azure_eventgrid {
					endpoint https://beyond-ms-grid.westeurope-1.eventgrid.azure.net
					topic_name demo-az-storage-account-feed
					subscription_name test
					credential {
						access_key {env.{env.AZ_EVENT_GRID_ACCOUNT_KEY}}
					}
				}
				transform <<EOF
				root.payload = this.payload.data_base64.decode("base64").parse_json()
				root.subject = this.subject
				root.headers = this.headers
				EOF
				destination stream {
					name "AZ-STORAGE-ACCOUNT"
					prefix "azstorage"
				}
			}
			flow {
				source stream_consumer {
					stream "AZ-STORAGE-ACCOUNT"
					durable_name "storage-sync"
				}
				destination azure_eventgrid {
					endpoint https://beyond-ms-grid.westeurope-1.eventgrid.azure.net
					topic_name demo-custom
					event_type CloudEventSchemaV1_0
					credential {
						access_key {env.AZ_EVENT_GRID_ACCOUNT_KEY}
					}
				}
			}
		}
		account APP {
			jetstream
			map_subject "canary.req.>" {
				to "req.>" weight 50
				to "beta.req.>" weight 50
			}
			map_subject "pub.>" to "pub.srv-01.>"
			authorize {
				match token APP
				match type nats
				match host 127.0.0.1
				callout allow
			}
			authorize {
				match type websocket
				match username APP
				callout oauth2 {
					endpoint az-oidc-endpoint
					template {
						allow_sub "_INBOX.{oidc.session.email}.>"
						allow_sub "{oidc.session.email}.>"
						allow_pub ">"
						allow_resp
					}
				}
			}
		}
		account SYS {
			export_service "$SYS.REQ.SERVER.PING" to MONITORING
			authorize {
				match token SYS
				match host 127.0.0.1
				callout allow
			}
		}
		account MONITORING {
			import_service "$SYS.REQ.SERVER.PING" from SYS
			authorize {
				match token monitoring
				match host 127.0.0.1
				callout allow
			}
		}
		jetstream {
			unique_tag region
			max_disk 10GB
			max_memory 1GB
			store_dir ./tmp/jetstream/t2
		}
		nats_server {
			name srv-01
			tags region:eu-west-3
			tls local.quara-dev.com localhost
			http_port 8222
			system_account SYS
			debug
			metrics all
		}
		websocket_server {
			advertise ws.local.quara-dev.com:10443
			tls ws.local.quara-dev.com
		}
		mqtt_server {
			tls mqtt.local.quara-dev.com
		}
		leafnode_server {
			advertise leafnode.local.quara-dev.com:7422
			tls leafnode.local.quara-dev.com
		}
	}
	# Enable oauth2 authentication/authorization by creating an oauth2 endpoint.
	# An endpoint is configured with various options, including a store where oauth2 sessions are stored
	# and an identity provider (oidc provider) which is used to authenticate users.
	oauth2 {
		az-oidc-endpoint {
			store jetstream {
				name OAUTH2-SESSIONS
				account OAUTH2
			}
			display_debug
			cookie_secure
			cookie_name _oauth2_session
			cookie_http_only no
			cookie_domains .local.quara-dev.com
			email_domains *
			whitelist_domains .local.quara-dev.com
			extra_jwt_issuers https://sts.windows.net/96bdcf75-3f33-4995-966b-d8eddb861f9b/=4a224c79-38e2-487f-96e0-5f4febfa4ce1
			provider oidc {
				id azure-ad
				name "Azure AD"
				client_id "4a224c79-38e2-487f-96e0-5f4febfa4ce1"
				client_secret "{secret.oauth2-client-secret@az-kv-srv-01}"
				scope openid email profile
				login_url https://login.microsoftonline.com/96bdcf75-3f33-4995-966b-d8eddb861f9b/v2.0/oauth2/authorize
				redeem_url https://login.microsoftonline.com/96bdcf75-3f33-4995-966b-d8eddb861f9b/v2.0/oauth2/token
				profile_url https://graph.microsoft.com/v1.0/me
				oidc_issuer_url https://login.microsoftonline.com/96bdcf75-3f33-4995-966b-d8eddb861f9b/v2.0
				oidc_jwks_url https://login.microsoftonline.com/common/discovery/keys
				oidc_audience_claims aud
				oidc_email_claim email
				oidc_user_id_claim email
			}
		}
	}
	# Enable and configure an embedded OpenTelemetry Collector
	# A collector must have at least one receiver and one exporter used together
	# in one pipeline. Processors can be used to modify telemetry data before exporting it.
	# telemetry {
	# 	extension zpages
	# 	extension basicauth/client {
	# 		username {secret.quara-grafana-stack-id@az-kv-srv-01}
	# 		password {secret.quara-grafana-stack-token@az-kv-srv-01}
	# 	}
	# 	receiver hostmetrics {
	# 		collection_interval 10s
	# 		initial_delay 5s
	# 		root_path /
	# 		scrap cpu
	# 	}
	# 	receiver prometheus {
	# 		scrape_interval 10s
	# 		scrape_timeout 1s
	# 		# Scrap caddy and otel-collector metrics
	# 		scrape_config caddy-beyond localhost:2019
	# 		scrape_config otel-collector localhost:2020
	# 		# Scrap additional metrics
	# 		# ...
	# 	}
	# 	receiver otlp {
	# 		grpc 127.0.0.1:4317
	# 		http 127.0.0.1:4318
	# 	}
	# 	receiver filelog {
	# 		include "*.log"
	# 		resource {
	# 			environment_name dev
	# 			service_name caddy-beyond
	# 			server_name srv-01
	# 		}
	# 	}
	# 	exporter otlphttp {
	# 		endpoint https://otlp-gateway-prod-eu-west-3.grafana.net/otlp
	# 		authenticator basicauth/client
	# 	}
	# 	processor batch
	# 	processor attributes {
	# 		action insert {
	# 			key server_name
	# 			value srv-01
	# 		}
	# 	}
	# 	service {
	# 		metrics 127.0.0.1:2020
	# 		extensions zpages basicauth/client
	# 		trace_pipeline {
	# 			receivers otlp
	# 			processors batch attributes
	# 			exporters otlphttp
	# 		}
	# 		metric_pipeline {
	# 			receivers otlp prometheus hostmetrics
	# 			processors batch attributes
	# 			exporters otlphttp
	# 		}
	# 		log_pipeline {
	# 			receivers otlp filelog
	# 			processors batch attributes
	# 			exporters otlphttp
	# 		}
	# 	}
	# }
}

# Each website is defined by a block like this one
# This is a "dummy" website with static content
local.quara-dev.com {
	log
	authorize_with oauth2 az-oidc-endpoint
	respond "hello world"
}

# This is another website
# This is a "file server" website, which serves static files from a directory
admin.local.quara-dev.com {
	log
	authorize_with oauth2 az-oidc-endpoint
	root * ./www/index.html
	file_server
}

# This is another website
# This is a "reverse proxy" website, which forwards requests to another server
demo.local.quara-dev.com {
	log
	authorize_with oauth2 az-oidc-endpoint
	reverse_proxy {
		dynamic docker {
			container demo-nginx
			network mynetwork
			port 80
		}
	}
}

# Yet another website, which serves web server and NATS metrics
metrics.local.quara-dev.com {
	log
	# authorize_with oauth2 az-oidc-endpoint
	metrics
}
